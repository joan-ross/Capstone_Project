{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37d1847",
   "metadata": {},
   "source": [
    "# Cleaning and Consolidating the Data\n",
    "This is the overview of my process in cleaning the data and merging the datasets as well as a critique of my own process. \n",
    "The following is my psuedocode from when I started the cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0eff16e-1b84-4b1d-8e72-d6f7ae5e2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## i need to extract the street addresses from the sidewalk repair report \n",
    "# and match them up with the sidewalk condition report \n",
    "# to see if they match up so it confirms that the sidewalks that are in poor condition were tended to.\n",
    "\n",
    "## i need to line up the addresses (Name column) in pavement distress report \n",
    "# with the condition and sidewalk repair report and incl. the distress description\n",
    "#  to show what kind of weathering was happening in that area\n",
    "\n",
    "## effectively try to merge 3 data sets into one new data set with the relevant information >> unable to accomplish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362b1ef9-d404-4f68-adc9-040a92f39c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b9c55f-a595-4c9f-85b1-2cb05d48af7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     CONDITION      TYPE    Roadname System ID\n",
      "38        POOR  CONCRETE   E 10th ST     r4652\n",
      "88        POOR  CONCRETE   E 11th ST      r476\n",
      "103       POOR  CONCRETE   E 12th ST     q4757\n",
      "133       POOR     PAVER   E 13th ST     r4712\n",
      "139       POOR     PAVER   E 13th ST     r4740\n",
      "...        ...       ...         ...       ...\n",
      "6349      POOR     PAVER  W Wylie ST     p4438\n",
      "6350      POOR     PAVER  W Wylie ST     p4438\n",
      "6351      POOR     PAVER  W Wylie ST     p4438\n",
      "6352      POOR     PAVER  W Wylie ST     p4438\n",
      "6353      POOR     PAVER  W Wylie ST     p4438\n",
      "\n",
      "[260 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "sidewalk_condition_report = pd.read_csv(\"raw/2018_Sidewalk_Condition_Report.csv\",\n",
    "                                usecols= ['CONDITION',\n",
    "                                          'TYPE',\n",
    "                                          'Roadname',\n",
    "                                         'SECTIONID' ])\n",
    "sidewalk_conditions = sidewalk_condition_report.rename(columns={'SECTIONID':'System ID'})\n",
    "poor_sidewalks = sidewalk_conditions[sidewalk_conditions['CONDITION'] == 'POOR']\n",
    "print(poor_sidewalks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe019e9",
   "metadata": {},
   "source": [
    "1. Read in the csv file needed.\n",
    "2. Used the usecols command to filter the dataset on only columns I deemed necessary for my project. \n",
    "3. Renamed the SECTIONID column to System ID as to match the second dataset I use. This is so that I can join the two columns into one, as you can only merge two columns of the same values and name. \n",
    "4. Filtered the dataset to only show me the sidewalks in poor conditionas they would be the ones with greater priority. \n",
    "5. printed the dataset in it's current state for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f6b5fd3-7c12-453a-9741-5de016224a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "##sidewalk_repair_status = pd.read_csv(\"data/raw/2018_Sidewalk_Repair_and_Reconstruction_Projects.csv\",\n",
    "                                     ##usecols= ['ADDRESS'])\n",
    "##contains null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09cbaf",
   "metadata": {},
   "source": [
    "*This is the code for the unused dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18379a53-2480-4e31-8d18-7b06c4868e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      System ID  PCI              Distress Description\n",
      "0         m4544   54                    BLOCK CRACKING\n",
      "1         m4544   54                        WEATHERING\n",
      "2         m4544   54                    BLOCK CRACKING\n",
      "3         m4544   54                           POTHOLE\n",
      "4         m4544   54                 PATCH/UTILITY CUT\n",
      "...         ...  ...                               ...\n",
      "14982     p4438   63                        WEATHERING\n",
      "14983    rcl357   41                ALLIGATOR CRACKING\n",
      "14984    rcl357   41                ALLIGATOR CRACKING\n",
      "14985    rcl357   41                        WEATHERING\n",
      "14986    rcl357   41  LONGITUDINAL/TRANSVERSE CRACKING\n",
      "\n",
      "[14987 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "distress_type = pd.read_csv(\"raw/2018_Pavement_Distress_Report.csv\",\n",
    "                            usecols= ['System ID',                  \n",
    "                                      'Distress Description',\n",
    "                                      'PCI'])\n",
    "##dtype = distress_type.rename(columns={'Name': 'Roadname'})\n",
    "print(distress_type)\n",
    "## contains null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5edf83",
   "metadata": {},
   "source": [
    "1. Read in the csv file needed.\n",
    "2. Used the usecols command to filter the dataset on only columns I deemed necessary for my project. \n",
    "3. printed the dataset in it's current state for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc90616-b791-4989-9819-23120e4f39f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     CONDITION      TYPE    Roadname System ID   PCI  \\\n",
      "0         POOR  CONCRETE   E 10th ST     r4652  70.0   \n",
      "1         POOR  CONCRETE   E 10th ST     r4652  70.0   \n",
      "2         POOR  CONCRETE   E 10th ST     r4652  70.0   \n",
      "3         POOR  CONCRETE   E 11th ST      r476  48.0   \n",
      "4         POOR  CONCRETE   E 11th ST      r476  48.0   \n",
      "...        ...       ...         ...       ...   ...   \n",
      "1429      POOR     PAVER  W Wylie ST     p4438  63.0   \n",
      "1430      POOR     PAVER  W Wylie ST     p4438  63.0   \n",
      "1431      POOR     PAVER  W Wylie ST     p4438  63.0   \n",
      "1432      POOR     PAVER  W Wylie ST     p4438  63.0   \n",
      "1433      POOR     PAVER  W Wylie ST     p4438  63.0   \n",
      "\n",
      "                  Distress Description  \n",
      "0                           WEATHERING  \n",
      "1                   ALLIGATOR CRACKING  \n",
      "2     LONGITUDINAL/TRANSVERSE CRACKING  \n",
      "3                    PATCH/UTILITY CUT  \n",
      "4     LONGITUDINAL/TRANSVERSE CRACKING  \n",
      "...                                ...  \n",
      "1429                ALLIGATOR CRACKING  \n",
      "1430                        WEATHERING  \n",
      "1431  LONGITUDINAL/TRANSVERSE CRACKING  \n",
      "1432  LONGITUDINAL/TRANSVERSE CRACKING  \n",
      "1433                        WEATHERING  \n",
      "\n",
      "[1427 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "## Failed attempts at joining the datasets:\n",
    "##dtype_sidewalks = poor_sidewalks.join(dtype, how = 'inner', on ='Roadname')\n",
    "##dtype_sidewalks = pd.concat([poor_sidewalks, dtype], join = 'outer')\n",
    "\n",
    "dtype_sidewalks = pd.merge(poor_sidewalks, distress_type, how = 'left', on= 'System ID')\n",
    "##dtypenull = dtype_sidewalks.isnull().any(axis=1)\n",
    "cleaned_data = dtype_sidewalks.dropna(axis =0, subset=['Distress Description'])\n",
    "##cd_nulls = cleaned_data.isnull().sum()\n",
    "print (cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a4a9d",
   "metadata": {},
   "source": [
    "*Failed attempts: Originally I was planning on joining the data based on the name of the road using the Roadname column. That did not seem to be as conclusive as the System ID approach which is why I went that route. Looking back, that seems rather obvious. The failed attempts showcase that the joins I was trying with that method did not work.*\n",
    "\n",
    "1. Left merged the two datasets on the System ID column\n",
    "2. Checked the dataset for any null values. There were 7 found in the Distress Description column.\n",
    "3. Dropped the rows that contained the null values as they would not be useful for this project.\n",
    "4. Checked to see if there were any more null values, or if I was succesful in cleaning the data.\n",
    "5. Printed the dataframe to check my work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f4f3ba-7155-400a-960e-326184e69a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('Sidewalk_Conditions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a6ec9",
   "metadata": {},
   "source": [
    "Exported the cleaned dataframe to it's own csv file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
